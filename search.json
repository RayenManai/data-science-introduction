[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Week\nTopic Resume\nR Lab1 keywords\n\n\n\n\n1\n- Intro to Data Science\nprogramming-basics\n\n\n2\n- Data and visualization  - Grammar of data wrangling\ntibbles, exploratory-data-analysis (variation vs covariation), visualization-basics, isolating\n\n\n3\n- Experiments; Explore data\nbar-charts"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction to Data Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ncan be opened from R Studio with:\nlearnr::run_tutorial(\"exploratory-data-analysis\",\"idsst.rtutorials\")\n\n\n\n\nIndex\n\n\n\n\n\nQuarto Files:\n\n 01-meet-the-penguins hotels-template\n\n\n↩︎"
  },
  {
    "objectID": "Lecture-summaries/Intro-to-data-science.html",
    "href": "Lecture-summaries/Intro-to-data-science.html",
    "title": "",
    "section": "",
    "text": "In statistical experiments,the treatment group is the group that receives the intervention or treatment being studied, while the control group is similar but does not receive the treatment. This setup helps researchers compare the effects of the treatment against no treatment or against a standard treatment, if one exists.\n\n\n\nIf the difference is quite large, it is more believable that the difference is real. Conclusion: We need statistical tools to determine quantitatively if the difference is so large that we should reject the notion that it was due to chance.\n\n\n\n\n\n\nTypes of Variables\n\n\nThe type of a variable is one of the following:\n\n\nNumerical: Variable can take a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values.\n\n\nCategorical: Variable has a finite number of values, which are categories (called levels), and it is not sensible to add, subtract, or take averages with those values.\n\n\n\n\nCategorical variables can be further distinguished as:\n\n\nordinal: the levels of the variable have a natural ordering, or\n\n\nnominal: the levels of the variable don’t have a natural ordering\n\n\n\n\n\n\n\n\nWhen two variables show some connection with one another, they are called associated or dependent variables. Conclusion: If two variables are not associated, i.e., there is no evident connection between them, they are said to be independent.\n\n\n\n\n\nWhen we suspect one variable might causally affect another, we label the first variable the explanatory variable and the second the response variable.\n\n\n\n\n\nLabeling variables as explanatory and response does not guarantee the relationship between the two is causal, even if an association is identified between the two variables.\n\n\n\n\n\n\nWouldn’t it be better to just include everyone and “sample” the entire population? This is called a census. There are problems with taking a census: 1. It can be difficult to complete a census: there always seem to be some subjects who are hard to locate or hard to measure. And these difficult-to-find subjects may have certain characteristics that distinguish them from the rest of the population. 2. Populations rarely stand still. Even if you could take a census, the population changes con- stantly, so it’s never possible to get a perfect measure. 3. Taking a census may be more complex than sampling.\n\n\n\nExploratory analysis: You taste a spoonful of soup and decide the spoonful you tasted isn’t salty enough.\nInference: You generalize and conclude that your entire soup needs salt.\n\n\n\n\n\n\n\nNon-response: If only a small fraction of the randomly sampled people chooses to respond to a survey, the sample may no longer be representative of the population.\n\n\nVoluntary response: The sample consists of people who volunteer to respond, because they have strong opinions on the issue. Such a sample will also not be representative of the population\n\n\nConvenience sample: Individuals who are easily accessible are more likely to be included in the sample.\n\n\n\n\n\n\nShow Answer\n\ni and iii\n\n\n\n\n\n\n\n\n\n\nIf researchers collect data in a way that does not directly interfere with how the data arise, i.e., they merely “observe”, we call it an observational study. In this case, only a relationship between the explanatory and the response variables can be established.\n\n\nIf researchers randomly assign subjects to various treatments in order to establish causal connections between the explanatory and response variables, we call it an experiment.\n\n\n\n\n\n Extraneous variables, that affect both the explanatory and the response variable and that make it seem like there is a relationship between the two, are called confounding variables.\n\nExample: A study found a rather strong correlation between the ice cream sales and the number of shark attacks for a number of beaches that were sampled.\nConclusion: Increasing ice cream sales causes more shark attacks (sharks like eating people full of ice cream). Better explanation: The confounding variable is temperature. Warmer temperatures cause ice cream sales to go up. Warmer temperatures also bring more people to the beaches, increasing the chances of shark attacks.\n\n\n\n\nIf observational data are not collected in a random framework from a population, these statistical methods, i.e.,the estimates and errors associated with the estimates, are not reliable.\n\n\n\n\nSimple random sampling\n\nRandomly select cases from the population without implied connection between the selected points. A sample is called a simple random sample if each case in the population has an equal chance of being included in the final sample.\n\n  \n\n\n\nStratified sampling\n\nSimilar cases from the population are grouped into so-called strata. Afterward, a simple random sample is taken from each stratum. Stratified sampling is especially useful when the cases in each stratum are very similar in terms of the outcome of interest.\n\n  \n\n\n\nCluster sampling\n\nClusters are usually not made up of homogeneous observations. We take a simple random sample of clusters, and then sample all observations in that cluster.\n\n \n\n\n\nMultistage sampling\n\nClusters are usually not made up of homogeneous observations. We take a simple random sample of clusters, and then take a simple random sample within each sampled cluster.\n\n  \n\nCluster or multistage sampling can be more economical than the other sampling techniques. Also, unlike stratified samples, they are most useful when there is a large case-to-case variability within a cluster, but the clusters themselves do not look very different. ##### examples: A city council has requested a household survey be conducted in a suburban area of their city. The area is broken into many distinct and unique neighborhoods, some including large homes and some with only apartments. Which approach would likely be the least effective?\n\n\n\nShow Answer\n\nWe can think of the different neighborhoods as different clusters/strata. Since they are distinct but homogeneous within, cluster samlping would be the least effective.\n\n\nOn a large college campus first-year students and sophomores live in dorms located on the eastern part of the campus and juniors and seniors live in dorms located on the western part of the campus. Suppose you want to collect student opinions on a new housing structure the college administration is proposing and you want to make sure your survey equally represents opinions from students from all years.\na) What type of study is this? b) Suggest a sampling strategy for carrying out this study\n\n\nShow Answer\n\n\nobservational study\n\nTo ensure students from each year are reasonably represented, we might choose to randomly sample a fixed number of students, say 60, from each part of the campus (east and west). Since a random sample of fixed size was taken within each part in this scenario, this represents a stratified sampling.\n\n\n\n\n\n\n\n\nControl: Compare the treatment of interest to a control group.\nRandomize: Randomly assign subjects to treatments, and randomly sample from the popula- tion whenever possible.\nReplicate: Within a study, replicate by collecting a sufficiently large sample. Or replicate the entire study.\nBlock: If there are variables that are known or suspected to affect the response variable, first group subjects into blocks based on these variables, and then randomize cases within each block to treatment groups\n\n\n\n\nIt is suspected that energy gels might affect pro and amateur athletes differently, therefore we block for pro status: 1. divide the sample into pro and amateur 2. randomly assign pro athletes to treatment and control groups 3. randomly assign amateur athletes to treatment and control groups 4. pro/amateur status is equally represented in the resulting treatment and control groups - Treatment: energy gel - Control: no energy gel\n\nA study is designed to test the effect of light level and noise level on the exam performance of students. The researcher also believes that light and noise levels might affect males and females differently, so she wants to make sure both genders are equally represented in each group:\n\n\nShow Answer\n\nThere are 2 explanatory variables (light and noise), 1 blocking variable (gender), and 1 response variable (exam performance)\n\n\nTreatment variables are conditions we can impose on the experimental units.\n\n\nBlocking variables are characteristics that the experimental units come with, that we would like to control for. Blocking is like stratifying, except used in experimental settings when randomly assigning, as opposed to sampling.\n\n\n\n\n\nPlacebo: fake treatment, often used as the control group for medical studies\nPlacebo effect: experimental units showing improvement simply because they believe they are receiving a special treatment\nBlinding: when experimental units do not know whether they are in the control or treatment group\nDouble-blind: when both the experimental units and the researchers who interact with the patients do not know who is in the control and who is in the treatment group\n\n\n\nWhat is the main difference between observational studies and experiments?\n\nMost experiments use random assignment while observational studies do not."
  },
  {
    "objectID": "Lecture-summaries/Intro-to-data-science.html#intro-to-data",
    "href": "Lecture-summaries/Intro-to-data-science.html#intro-to-data",
    "title": "",
    "section": "",
    "text": "In statistical experiments,the treatment group is the group that receives the intervention or treatment being studied, while the control group is similar but does not receive the treatment. This setup helps researchers compare the effects of the treatment against no treatment or against a standard treatment, if one exists.\n\n\n\nIf the difference is quite large, it is more believable that the difference is real. Conclusion: We need statistical tools to determine quantitatively if the difference is so large that we should reject the notion that it was due to chance.\n\n\n\n\n\n\nTypes of Variables\n\n\nThe type of a variable is one of the following:\n\n\nNumerical: Variable can take a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values.\n\n\nCategorical: Variable has a finite number of values, which are categories (called levels), and it is not sensible to add, subtract, or take averages with those values.\n\n\n\n\nCategorical variables can be further distinguished as:\n\n\nordinal: the levels of the variable have a natural ordering, or\n\n\nnominal: the levels of the variable don’t have a natural ordering\n\n\n\n\n\n\n\n\nWhen two variables show some connection with one another, they are called associated or dependent variables. Conclusion: If two variables are not associated, i.e., there is no evident connection between them, they are said to be independent.\n\n\n\n\n\nWhen we suspect one variable might causally affect another, we label the first variable the explanatory variable and the second the response variable.\n\n\n\n\n\nLabeling variables as explanatory and response does not guarantee the relationship between the two is causal, even if an association is identified between the two variables.\n\n\n\n\n\n\nWouldn’t it be better to just include everyone and “sample” the entire population? This is called a census. There are problems with taking a census: 1. It can be difficult to complete a census: there always seem to be some subjects who are hard to locate or hard to measure. And these difficult-to-find subjects may have certain characteristics that distinguish them from the rest of the population. 2. Populations rarely stand still. Even if you could take a census, the population changes con- stantly, so it’s never possible to get a perfect measure. 3. Taking a census may be more complex than sampling.\n\n\n\nExploratory analysis: You taste a spoonful of soup and decide the spoonful you tasted isn’t salty enough.\nInference: You generalize and conclude that your entire soup needs salt.\n\n\n\n\n\n\n\nNon-response: If only a small fraction of the randomly sampled people chooses to respond to a survey, the sample may no longer be representative of the population.\n\n\nVoluntary response: The sample consists of people who volunteer to respond, because they have strong opinions on the issue. Such a sample will also not be representative of the population\n\n\nConvenience sample: Individuals who are easily accessible are more likely to be included in the sample.\n\n\n\n\n\n\nShow Answer\n\ni and iii\n\n\n\n\n\n\n\n\n\n\nIf researchers collect data in a way that does not directly interfere with how the data arise, i.e., they merely “observe”, we call it an observational study. In this case, only a relationship between the explanatory and the response variables can be established.\n\n\nIf researchers randomly assign subjects to various treatments in order to establish causal connections between the explanatory and response variables, we call it an experiment.\n\n\n\n\n\n Extraneous variables, that affect both the explanatory and the response variable and that make it seem like there is a relationship between the two, are called confounding variables.\n\nExample: A study found a rather strong correlation between the ice cream sales and the number of shark attacks for a number of beaches that were sampled.\nConclusion: Increasing ice cream sales causes more shark attacks (sharks like eating people full of ice cream). Better explanation: The confounding variable is temperature. Warmer temperatures cause ice cream sales to go up. Warmer temperatures also bring more people to the beaches, increasing the chances of shark attacks.\n\n\n\n\nIf observational data are not collected in a random framework from a population, these statistical methods, i.e.,the estimates and errors associated with the estimates, are not reliable.\n\n\n\n\nSimple random sampling\n\nRandomly select cases from the population without implied connection between the selected points. A sample is called a simple random sample if each case in the population has an equal chance of being included in the final sample.\n\n  \n\n\n\nStratified sampling\n\nSimilar cases from the population are grouped into so-called strata. Afterward, a simple random sample is taken from each stratum. Stratified sampling is especially useful when the cases in each stratum are very similar in terms of the outcome of interest.\n\n  \n\n\n\nCluster sampling\n\nClusters are usually not made up of homogeneous observations. We take a simple random sample of clusters, and then sample all observations in that cluster.\n\n \n\n\n\nMultistage sampling\n\nClusters are usually not made up of homogeneous observations. We take a simple random sample of clusters, and then take a simple random sample within each sampled cluster.\n\n  \n\nCluster or multistage sampling can be more economical than the other sampling techniques. Also, unlike stratified samples, they are most useful when there is a large case-to-case variability within a cluster, but the clusters themselves do not look very different. ##### examples: A city council has requested a household survey be conducted in a suburban area of their city. The area is broken into many distinct and unique neighborhoods, some including large homes and some with only apartments. Which approach would likely be the least effective?\n\n\n\nShow Answer\n\nWe can think of the different neighborhoods as different clusters/strata. Since they are distinct but homogeneous within, cluster samlping would be the least effective.\n\n\nOn a large college campus first-year students and sophomores live in dorms located on the eastern part of the campus and juniors and seniors live in dorms located on the western part of the campus. Suppose you want to collect student opinions on a new housing structure the college administration is proposing and you want to make sure your survey equally represents opinions from students from all years.\na) What type of study is this? b) Suggest a sampling strategy for carrying out this study\n\n\nShow Answer\n\n\nobservational study\n\nTo ensure students from each year are reasonably represented, we might choose to randomly sample a fixed number of students, say 60, from each part of the campus (east and west). Since a random sample of fixed size was taken within each part in this scenario, this represents a stratified sampling.\n\n\n\n\n\n\n\n\nControl: Compare the treatment of interest to a control group.\nRandomize: Randomly assign subjects to treatments, and randomly sample from the popula- tion whenever possible.\nReplicate: Within a study, replicate by collecting a sufficiently large sample. Or replicate the entire study.\nBlock: If there are variables that are known or suspected to affect the response variable, first group subjects into blocks based on these variables, and then randomize cases within each block to treatment groups\n\n\n\n\nIt is suspected that energy gels might affect pro and amateur athletes differently, therefore we block for pro status: 1. divide the sample into pro and amateur 2. randomly assign pro athletes to treatment and control groups 3. randomly assign amateur athletes to treatment and control groups 4. pro/amateur status is equally represented in the resulting treatment and control groups - Treatment: energy gel - Control: no energy gel\n\nA study is designed to test the effect of light level and noise level on the exam performance of students. The researcher also believes that light and noise levels might affect males and females differently, so she wants to make sure both genders are equally represented in each group:\n\n\nShow Answer\n\nThere are 2 explanatory variables (light and noise), 1 blocking variable (gender), and 1 response variable (exam performance)\n\n\nTreatment variables are conditions we can impose on the experimental units.\n\n\nBlocking variables are characteristics that the experimental units come with, that we would like to control for. Blocking is like stratifying, except used in experimental settings when randomly assigning, as opposed to sampling.\n\n\n\n\n\nPlacebo: fake treatment, often used as the control group for medical studies\nPlacebo effect: experimental units showing improvement simply because they believe they are receiving a special treatment\nBlinding: when experimental units do not know whether they are in the control or treatment group\nDouble-blind: when both the experimental units and the researchers who interact with the patients do not know who is in the control and who is in the treatment group\n\n\n\nWhat is the main difference between observational studies and experiments?\n\nMost experiments use random assignment while observational studies do not."
  },
  {
    "objectID": "Lecture-summaries/Grammar-of-data-wrangling.html",
    "href": "Lecture-summaries/Grammar-of-data-wrangling.html",
    "title": "",
    "section": "",
    "text": "In this chapter, the dplyr package of tidyverse is introduced. It is a grammar of data manipulation, providing a consistent set of verbs (=functions) that help you solve the most common data manipulation challenges.\n\nThe main verbs are:\n\nselect: pick columns by name,\narrange: reorder rows,\nfilter: pick rows matching criteria,\nmutate: add new variables,\nsummarise: reduce variables to values.\n\n\n For all dplyr functions:\n\n\nthe first argument is always a data frame\n\n\nthe subsequent arguments say what to do with that data frame\n\n\nalways return a data frame\n\n\ndon’t modify in place\n\n\n\n\n\nThe tidyverse toolbox for data input/output is the readr package. It is one of the core tidyverse packages loaded when loading the tidyverse. Since we already loaded the tidyverse, readr is ready for usage.\nThe most general function for reading in data is read_delim(). Several variants with respect to the relevant field separator exist to make our lives easier. In our case, it is a comma. Therefore, we use read_csv() (in case of a semicolon, it would be read_csv2()).\nhotels &lt;- read_csv(\"/data/hotels.csv\")\n\n\nselect(dataframe, …variable)\nselect(\n    hotels,\n    lead_time\n)\nIn this example, hotels and the output of select() are a tibble, which is a special kind of data frame. In particular, it prints information about the dimension of the data and the type of the variables. Most of the time we will work with tibbles.\n\n\n\n\nBy default, arrange() will order the entries ascendingly.\n\narrange(\n    select(hotels\n           hotel, lead_time),\n    desc(lead_time)\n)\nA more efficient way of combining several steps into one command is to use Pipes\n\n\n\n\nThe pipe operator |&gt; : a technique for passing information from one process to another.\n\nBy default, the object on the left is passed as the first argument of the function on the right. The native pipe also has a placeholder option, allowing objects to be passed to a specific argument.\nhotels |&gt;\n    select(hotel, lead_time) |&gt;\n    arrange(desc(lead_time))\nRemark: dplyr knows it’s own pipe operator %&gt;%, which is actually implemented in the package magrittr. This operator is older but has the drawback of working only in an “extended tidyverse”\n\n\n\nWe use |&gt; mainly in dplyr pipelines: we pipe the output of the previous line of code as the first input of the next line of code We use + in ggplot2 plots for “layering”: we create the plot in layers, separated by +\n\n\n\n\nstarts_with(): Starts with a prefix\nends_with(): Ends with a suffix\ncontains(): Contains a literal string\nnum_range(): Matches a numerical range like x01, x02, x03\none_of(): Matches variable names in a character vector\neverything(): Matches all variables\nlast_col(): Select last variable, possibly with an offset\nmatches(): Matches a regular expression (a sequence of symbols/characters expressing a string/pattern to be searched for within text)\n\nhotels |&gt;\n    select(starts_with(\"arrival\"))\n\n\n\n\nThe subsequent filter() arguments specify conditions that need to be fulfilled by a row (= an ob- servation) to become part of the output. We can specify multiple conditions, which will be combined with an &.\n\nhotels |&gt;\n    filter(\n        adults == 0,\n        children &gt;= 1\n    ) |&gt;\n    select(adults, babies, children)\n\nIf two (or more) conditions should be combined with an “or”, we must do this explicitly using the | operator.\n\nhotels |&gt;\n    filter(\n        adults == 0,\n        children &gt;= 1 | babies &gt;= 1 \n    ) |&gt;\n    select(adults, babies, children)\n\nIn some cases, we might be interested in the unique observations of a variable. That’s when we want to use the distinct() function Combining distinct() with arrange() leads to a friendlier output to read through the fact being ordered.\n\nhotels |&gt;\n    distinct(hotel,\n             market_segment) |&gt;  #distinct combinations\n    arrange(hotel, market_segment)\n\n\n\n\nIf we know which rows to extract, slice() can do the job.\n\nhotels |&gt;\n    slice(1:5) # first five\nslice_head(df, n = 1, by = group) # first row from each group.\nslice_tail(df, n = 1, by = group) # last row in each group.\nslice_min(df, x, n = 1) # smallest value of column x.\nslice_max(df, x, n = 1) # largest value of column x.\nslice_sample(df, n = 1) # one random row.\n\n\n\nImagine that it’s not essential for the analysis to distinguish between children and babies. Instead, we would like to have the number of little ones (children or babies) staying in the room.\nhotels |&gt;\n    mutate(little_ones= children + babies) |&gt;\n    select(children, babies, little_ones) |&gt;\n    arrange(desc(little_ones))\n\ncount the combinations of how many bookings for all possible combinations of hotel and little ones:\n\nhotels |&gt;\n    mutate(little_ones = children + babies) |&gt;\n    count(hotel, little_ones) |&gt;  #count creates a further column n\n    mutate(prop = n / sum(n))\n\n\n\n\n\nGood code styling is not necessary but is highly beneficial. The readability of your code is something you benefit from the most.\n\nThe styler package provides functions for converting code to follow a chosen style.\nBy using the styler addin for the RStudio IDE, it makes it even easier than that."
  },
  {
    "objectID": "Lecture-summaries/Grammar-of-data-wrangling.html#grammar-of-data-wrangling",
    "href": "Lecture-summaries/Grammar-of-data-wrangling.html#grammar-of-data-wrangling",
    "title": "",
    "section": "",
    "text": "In this chapter, the dplyr package of tidyverse is introduced. It is a grammar of data manipulation, providing a consistent set of verbs (=functions) that help you solve the most common data manipulation challenges.\n\nThe main verbs are:\n\nselect: pick columns by name,\narrange: reorder rows,\nfilter: pick rows matching criteria,\nmutate: add new variables,\nsummarise: reduce variables to values.\n\n\n For all dplyr functions:\n\n\nthe first argument is always a data frame\n\n\nthe subsequent arguments say what to do with that data frame\n\n\nalways return a data frame\n\n\ndon’t modify in place\n\n\n\n\n\nThe tidyverse toolbox for data input/output is the readr package. It is one of the core tidyverse packages loaded when loading the tidyverse. Since we already loaded the tidyverse, readr is ready for usage.\nThe most general function for reading in data is read_delim(). Several variants with respect to the relevant field separator exist to make our lives easier. In our case, it is a comma. Therefore, we use read_csv() (in case of a semicolon, it would be read_csv2()).\nhotels &lt;- read_csv(\"/data/hotels.csv\")\n\n\nselect(dataframe, …variable)\nselect(\n    hotels,\n    lead_time\n)\nIn this example, hotels and the output of select() are a tibble, which is a special kind of data frame. In particular, it prints information about the dimension of the data and the type of the variables. Most of the time we will work with tibbles.\n\n\n\n\nBy default, arrange() will order the entries ascendingly.\n\narrange(\n    select(hotels\n           hotel, lead_time),\n    desc(lead_time)\n)\nA more efficient way of combining several steps into one command is to use Pipes\n\n\n\n\nThe pipe operator |&gt; : a technique for passing information from one process to another.\n\nBy default, the object on the left is passed as the first argument of the function on the right. The native pipe also has a placeholder option, allowing objects to be passed to a specific argument.\nhotels |&gt;\n    select(hotel, lead_time) |&gt;\n    arrange(desc(lead_time))\nRemark: dplyr knows it’s own pipe operator %&gt;%, which is actually implemented in the package magrittr. This operator is older but has the drawback of working only in an “extended tidyverse”\n\n\n\nWe use |&gt; mainly in dplyr pipelines: we pipe the output of the previous line of code as the first input of the next line of code We use + in ggplot2 plots for “layering”: we create the plot in layers, separated by +\n\n\n\n\nstarts_with(): Starts with a prefix\nends_with(): Ends with a suffix\ncontains(): Contains a literal string\nnum_range(): Matches a numerical range like x01, x02, x03\none_of(): Matches variable names in a character vector\neverything(): Matches all variables\nlast_col(): Select last variable, possibly with an offset\nmatches(): Matches a regular expression (a sequence of symbols/characters expressing a string/pattern to be searched for within text)\n\nhotels |&gt;\n    select(starts_with(\"arrival\"))\n\n\n\n\nThe subsequent filter() arguments specify conditions that need to be fulfilled by a row (= an ob- servation) to become part of the output. We can specify multiple conditions, which will be combined with an &.\n\nhotels |&gt;\n    filter(\n        adults == 0,\n        children &gt;= 1\n    ) |&gt;\n    select(adults, babies, children)\n\nIf two (or more) conditions should be combined with an “or”, we must do this explicitly using the | operator.\n\nhotels |&gt;\n    filter(\n        adults == 0,\n        children &gt;= 1 | babies &gt;= 1 \n    ) |&gt;\n    select(adults, babies, children)\n\nIn some cases, we might be interested in the unique observations of a variable. That’s when we want to use the distinct() function Combining distinct() with arrange() leads to a friendlier output to read through the fact being ordered.\n\nhotels |&gt;\n    distinct(hotel,\n             market_segment) |&gt;  #distinct combinations\n    arrange(hotel, market_segment)\n\n\n\n\nIf we know which rows to extract, slice() can do the job.\n\nhotels |&gt;\n    slice(1:5) # first five\nslice_head(df, n = 1, by = group) # first row from each group.\nslice_tail(df, n = 1, by = group) # last row in each group.\nslice_min(df, x, n = 1) # smallest value of column x.\nslice_max(df, x, n = 1) # largest value of column x.\nslice_sample(df, n = 1) # one random row.\n\n\n\nImagine that it’s not essential for the analysis to distinguish between children and babies. Instead, we would like to have the number of little ones (children or babies) staying in the room.\nhotels |&gt;\n    mutate(little_ones= children + babies) |&gt;\n    select(children, babies, little_ones) |&gt;\n    arrange(desc(little_ones))\n\ncount the combinations of how many bookings for all possible combinations of hotel and little ones:\n\nhotels |&gt;\n    mutate(little_ones = children + babies) |&gt;\n    count(hotel, little_ones) |&gt;  #count creates a further column n\n    mutate(prop = n / sum(n))\n\n\n\n\n\nGood code styling is not necessary but is highly beneficial. The readability of your code is something you benefit from the most.\n\nThe styler package provides functions for converting code to follow a chosen style.\nBy using the styler addin for the RStudio IDE, it makes it even easier than that."
  },
  {
    "objectID": "Quarto-files/hotels-template.html",
    "href": "Quarto-files/hotels-template.html",
    "title": "Hotel bookings - data wrangling",
    "section": "",
    "text": "library(tidyverse)\nThe dataset used in this exercise comes from Tidy Tuesday. It’s data about hotel bookings.\nSince the data is not contained in some package, we have to read in the data on our own. To do so, we use the read_csv() function from the readr (part of the tidyverse) package.\nhotels &lt;- read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\nRows: 119390 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (13): hotel, arrival_date_month, meal, country, market_segment, distrib...\ndbl  (18): is_canceled, lead_time, arrival_date_year, arrival_date_week_numb...\ndate  (1): reservation_status_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRemark: The read_csv() function works not only if the data file is stored on your computer, you can also use it to read csv files that are available on a web server.\nTask 1\nsummary(hotels)\n\n    hotel            is_canceled       lead_time   arrival_date_year\n Length:119390      Min.   :0.0000   Min.   :  0   Min.   :2015     \n Class :character   1st Qu.:0.0000   1st Qu.: 18   1st Qu.:2016     \n Mode  :character   Median :0.0000   Median : 69   Median :2016     \n                    Mean   :0.3704   Mean   :104   Mean   :2016     \n                    3rd Qu.:1.0000   3rd Qu.:160   3rd Qu.:2017     \n                    Max.   :1.0000   Max.   :737   Max.   :2017     \n                                                                    \n arrival_date_month arrival_date_week_number arrival_date_day_of_month\n Length:119390      Min.   : 1.00            Min.   : 1.0             \n Class :character   1st Qu.:16.00            1st Qu.: 8.0             \n Mode  :character   Median :28.00            Median :16.0             \n                    Mean   :27.17            Mean   :15.8             \n                    3rd Qu.:38.00            3rd Qu.:23.0             \n                    Max.   :53.00            Max.   :31.0             \n                                                                      \n stays_in_weekend_nights stays_in_week_nights     adults      \n Min.   : 0.0000         Min.   : 0.0         Min.   : 0.000  \n 1st Qu.: 0.0000         1st Qu.: 1.0         1st Qu.: 2.000  \n Median : 1.0000         Median : 2.0         Median : 2.000  \n Mean   : 0.9276         Mean   : 2.5         Mean   : 1.856  \n 3rd Qu.: 2.0000         3rd Qu.: 3.0         3rd Qu.: 2.000  \n Max.   :19.0000         Max.   :50.0         Max.   :55.000  \n                                                              \n    children           babies              meal             country         \n Min.   : 0.0000   Min.   : 0.000000   Length:119390      Length:119390     \n 1st Qu.: 0.0000   1st Qu.: 0.000000   Class :character   Class :character  \n Median : 0.0000   Median : 0.000000   Mode  :character   Mode  :character  \n Mean   : 0.1039   Mean   : 0.007949                                        \n 3rd Qu.: 0.0000   3rd Qu.: 0.000000                                        \n Max.   :10.0000   Max.   :10.000000                                        \n NA's   :4                                                                  \n market_segment     distribution_channel is_repeated_guest\n Length:119390      Length:119390        Min.   :0.00000  \n Class :character   Class :character     1st Qu.:0.00000  \n Mode  :character   Mode  :character     Median :0.00000  \n                                         Mean   :0.03191  \n                                         3rd Qu.:0.00000  \n                                         Max.   :1.00000  \n                                                          \n previous_cancellations previous_bookings_not_canceled reserved_room_type\n Min.   : 0.00000       Min.   : 0.0000                Length:119390     \n 1st Qu.: 0.00000       1st Qu.: 0.0000                Class :character  \n Median : 0.00000       Median : 0.0000                Mode  :character  \n Mean   : 0.08712       Mean   : 0.1371                                  \n 3rd Qu.: 0.00000       3rd Qu.: 0.0000                                  \n Max.   :26.00000       Max.   :72.0000                                  \n                                                                         \n assigned_room_type booking_changes   deposit_type          agent          \n Length:119390      Min.   : 0.0000   Length:119390      Length:119390     \n Class :character   1st Qu.: 0.0000   Class :character   Class :character  \n Mode  :character   Median : 0.0000   Mode  :character   Mode  :character  \n                    Mean   : 0.2211                                        \n                    3rd Qu.: 0.0000                                        \n                    Max.   :21.0000                                        \n                                                                           \n   company          days_in_waiting_list customer_type           adr         \n Length:119390      Min.   :  0.000      Length:119390      Min.   :  -6.38  \n Class :character   1st Qu.:  0.000      Class :character   1st Qu.:  69.29  \n Mode  :character   Median :  0.000      Mode  :character   Median :  94.58  \n                    Mean   :  2.321                         Mean   : 101.83  \n                    3rd Qu.:  0.000                         3rd Qu.: 126.00  \n                    Max.   :391.000                         Max.   :5400.00  \n                                                                             \n required_car_parking_spaces total_of_special_requests reservation_status\n Min.   :0.00000             Min.   :0.0000            Length:119390     \n 1st Qu.:0.00000             1st Qu.:0.0000            Class :character  \n Median :0.00000             Median :0.0000            Mode  :character  \n Mean   :0.06252             Mean   :0.5714                              \n 3rd Qu.:0.00000             3rd Qu.:1.0000                              \n Max.   :8.00000             Max.   :5.0000                              \n                                                                         \n reservation_status_date\n Min.   :2014-10-17     \n 1st Qu.:2016-02-01     \n Median :2016-08-07     \n Mean   :2016-07-30     \n 3rd Qu.:2017-02-08     \n Max.   :2017-09-14\nTask 2\nhotels |&gt;\n  filter(\n    country ____ \"USA\", \n    lead_time ____ ____\n    )\nTask 3\nhotels |&gt;\n  filter(\n    children [AT LEAST] 1 [OR] babies [AT LEAST] 1\n    )\nTask 4\n# add code here\n# pay attention to correctness and code style\n# add code here\n# pay attention to correctness and code style\nTask 5\n# add code here\n# pay attention to correctness and code style\nTask 6\n# add code here\n# pay attention to correctness and code style"
  },
  {
    "objectID": "Quarto-files/hotels-template.html#section",
    "href": "Quarto-files/hotels-template.html#section",
    "title": "Hotel bookings - data wrangling",
    "section": "—————————–",
    "text": "—————————–"
  },
  {
    "objectID": "Quarto-files/hotels-template.html#data-dictionary",
    "href": "Quarto-files/hotels-template.html#data-dictionary",
    "title": "Hotel bookings - data wrangling",
    "section": "Data dictionary",
    "text": "Data dictionary\nBelow is the full data dictionary. Note that it is long (there are lots of variables in the data), but we will be using a limited set of the variables for our analysis.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nhotel\ncharacter\nHotel (H1 = Resort Hotel or H2 = City Hotel)\n\n\nis_canceled\ndouble\nValue indicating if the booking was canceled (1) or not (0)\n\n\nlead_time\ndouble\nNumber of days that elapsed between the entering date of the booking into the PMS and the arrival date\n\n\narrival_date_year\ndouble\nYear of arrival date\n\n\narrival_date_month\ncharacter\nMonth of arrival date\n\n\narrival_date_week_number\ndouble\nWeek number of year for arrival date\n\n\narrival_date_day_of_month\ndouble\nDay of arrival date\n\n\nstays_in_weekend_nights\ndouble\nNumber of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\n\n\nstays_in_week_nights\ndouble\nNumber of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel\n\n\nadults\ndouble\nNumber of adults\n\n\nchildren\ndouble\nNumber of children\n\n\nbabies\ndouble\nNumber of babies\n\n\nmeal\ncharacter\nType of meal booked. Categories are presented in standard hospitality meal packages:  Undefined/SC – no meal package;BB – Bed & Breakfast;  HB – Half board (breakfast and one other meal – usually dinner);  FB – Full board (breakfast, lunch and dinner)\n\n\ncountry\ncharacter\nCountry of origin. Categories are represented in the ISO 3155–3:2013 format\n\n\nmarket_segment\ncharacter\nMarket segment designation. In categories, the term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\ndistribution_channel\ncharacter\nBooking distribution channel. The term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\nis_repeated_guest\ndouble\nValue indicating if the booking name was from a repeated guest (1) or not (0)\n\n\nprevious_cancellations\ndouble\nNumber of previous bookings that were cancelled by the customer prior to the current booking\n\n\nprevious_bookings_not_canceled\ndouble\nNumber of previous bookings not cancelled by the customer prior to the current booking\n\n\nreserved_room_type\ncharacter\nCode of room type reserved. Code is presented instead of designation for anonymity reasons\n\n\nassigned_room_type\ncharacter\nCode for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons\n\n\nbooking_changes\ndouble\nNumber of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation\n\n\ndeposit_type\ncharacter\nIndication on if the customer made a deposit to guarantee the booking. This variable can assume three categories:No Deposit – no deposit was made;Non Refund – a deposit was made in the value of the total stay cost;Refundable – a deposit was made with a value under the total cost of stay.\n\n\nagent\ncharacter\nID of the travel agency that made the booking\n\n\ncompany\ncharacter\nID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons\n\n\ndays_in_waiting_list\ndouble\nNumber of days the booking was in the waiting list before it was confirmed to the customer\n\n\ncustomer_type\ncharacter\nType of booking, assuming one of four categories:Contract - when the booking has an allotment or other type of contract associated to it;Group – when the booking is associated to a group;Transient – when the booking is not part of a group or contract, and is not associated to other transient booking;Transient-party – when the booking is transient, but is associated to at least other transient booking\n\n\nadr\ndouble\nAverage Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights\n\n\nrequired_car_parking_spaces\ndouble\nNumber of car parking spaces required by the customer\n\n\ntotal_of_special_requests\ndouble\nNumber of special requests made by the customer (e.g. twin bed or high floor)\n\n\nreservation_status\ncharacter\nReservation last status, assuming one of three categories:Canceled – booking was canceled by the customer;Check-Out – customer has checked in but already departed;No-Show – customer did not check-in and did inform the hotel of the reason why\n\n\nreservation_status_date\ndouble\nDate at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel"
  },
  {
    "objectID": "Quarto-files/01-meet-the-penguins.html",
    "href": "Quarto-files/01-meet-the-penguins.html",
    "title": "Meet the penguins",
    "section": "",
    "text": "Note: Exercise is based on work from Mine Çetinkaya-Rundel.\nFor this application exercise, we’ll use the tidyverse and palmerpenguins packages.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\n\nDemo: Add the chunk option message: false to the above chunk. See what happens.\nThe dataset we will visualize is called penguins. Let’s glimpse() at it.\nYour turn: Replace #add code here with the code for “glimpse”ing at the data penguins data frame – glimpse(penguins). Render the document and view the output.\n\n# add code here\n\nDemo: First, replace the blank below with the number of rows in the penguins data frame based on the output of the chunk below. Then, replace it with “inline code” and render again.\n\nnrow(penguins)\n\n[1] 344\n\n\nThere are ___ penguins in the penguins data frame.\nYou can use inline R code if you don’t want to enter the value by hand:\nThere are ___ penguins in the penguins data frame.\n\nInsert code chunk\nYour turn: Assign the value 2 to an object called x and multiply by 3.\nDemo: In the console, add 5 to the object x. What happens?"
  },
  {
    "objectID": "Lecture-summaries/Experiments-explore-data.html",
    "href": "Lecture-summaries/Experiments-explore-data.html",
    "title": "",
    "section": "",
    "text": "The used Data Set is: loans_full_schema which is contained in the openintro package and includes thousands of loans made through the Lending Club, which is a platform that allows individuals to lend to other individuals.\nWe will examine the relationship between the two variables - homeownership, which can take one of the values of rent, mortgage (owns but has a mortgage), or own, and - application_type, which indicates whether the loan application was made with a partner (joint) or whether it was an individual application.\nThe data requires some data cleaning.\nloans &lt;- loans_full_schema |&gt;\n    mutate(\n        # lower case letters\n        homeownership = tolower(homeownership),\n        # pick new levels\n        homeownership = fct_relevel(\n        homeownership,\n        \"rent\", \"mortgage\", \"own\"\n    ),\n    application_type = fct_relevel(\n        as.character(application_type),\n        \"joint\", \"individual\"\n        )\n    )\n\n\n\nWe get the following absolute frequencies for homeownership and application_type.\nloans |&gt;\n    count(homeownership)\nloans |&gt;\n    count(application_type)\nInstead of absolute frequencies, we can compute the relative frequencies (proportions): \\(r_{j} = \\frac {f_{j}}{n}\\)\nloans |&gt;\n    count(homeownership) |&gt;\n    mutate(prop = n / sum(n))\n\n\n\n\nA bar chart is a common way to display the distribution of a single categorical variable. In ggplot2 we can use geom_bar() to create a bar chart.\n\nggplot(loans, aes(x = homeownership)) +\n    geom_bar(fill = \"gold\") +\n    labs(x = \"Homeownership\", y = \"Count\")\n\n\n\nIn the previous plot, we did not present the data as they are. In a preliminary step, absolute frequencies were calculated for homeownership and then these values were plotted.\n\nEach geom has its own set of variables to be calculated. For geom_bar() these are:\n\nComputed variables: These are calculated by the ‘stat’ part of layers and can be accessed with delayed evaluation. - after_stat(count) number of points in bin. - after_stat(prop) groupwise proportion\n\nThe help page of each geom function contains a list with all computed variables, where the first entry is the default computation.\n\nTo create a bar chart of relative frequencies (not absolute), we first have to apply the statistical transformation prop to the whole data set. after_stat(prop) computes groupwise proportions. The data contains three groups concerning homeownership. If we want to calculate proportions for each group with respect to the size of the whole dataset, we first have to assign a common group value (e.g., group = 1) for all three groups.\nggplot(loans,\n aes(x = homeownership,\n     y = after_stat(prop), group = 1)) +\n geom_bar(fill = \"gold\") +\n labs(x = \"Homeownership\")\n\n\n\n\n\n\n\nloans |&gt;\n    select(homeownership, application_type) |&gt;\n    table() \n\n#               application_type\n# homeownership joint individual\n#   rent        362 3496\n#   mortgage    950 3839\n#   own         183 1170\nWe can also add the marginal frequency distributions.\nloans |&gt;\n    select(homeownership, application_type) |&gt;\n    table() |&gt;\n    addmargins()\n\n#               application_type\n# homeownership joint individual Sum\n# rent          362     3496    3858\n# mortgage      950     3839    4789\n# own           183     1170    1353\n# Sum           1495    8505    10000\n\nContingency tables can also be computed using count().\n\n\nprop.table() converts a contingency table with absolute frequencies into a table with proportions.\n\nloans |&gt;\n    select(homeownership, application_type) |&gt;\n    table() |&gt;\n    prop.table()\n\n#               application_type\n# homeownership joint individual\n# rent          0.0362 0.3496\n# mortgage      0.0950 0.3839\n# own           0.0183 0.1170\nTo add row and column proportions, one can use the margin argument.\nFor row proportion (absolute frequencies divided by the row totals), we have to use margin=1\nloans |&gt;\n    select(homeownership, application_type) |&gt;\n    table() |&gt;\n    prop.table(margin = 1) |&gt;\n    addmargins()\n\n#                   application_type\n# homeownership joint     individual Sum\n# rent          0.0938310 0.9061690 1.0000000\n# mortgage      0.1983713 0.8016287 1.0000000\n# own           0.1352550 0.8647450 1.0000000\n# Sum           0.4274573 2.5725427 3.0000000\n\nRow and column proportions can also be thought of as conditional proportions as they tell us about the proportion of observations in a given level of a categorical variable conditional on the level of another categorical variable.\n\n\n\n\n\nWe can plot the distributions of two categorical variables simultaneously in a bar chart. Such charts are generally helpful to visualize the relationship between two categorical variables.\n\nggplot(loans, aes(x = homeownership, fill = application_type)) +\n    geom_bar() +\n    scale_fill_brewer(palette = \"Set1\") +\n    labs(\n        x = \"Homeownership\", y = \"Count\", fill = \"Application type\",\n        title = \"Stacked bar chart\"\n    )\nLoan applicants most often live in homes with mortgages. But it is not so easy to say how the different types of applications differ over the levels of homeownership. The stacked bar chart is most useful when it’s reasonable to assign one variable as the explanatory variable (here homeownership) and the other variable as the response (here application_type) since we are effectively grouping by one variable first and then breaking it down by the others.\nOne can vary the bars’ position with the position argument of geom_bar()\nggplot(loans, aes(x = homeownership, fill = application_type)) +\n geom_bar(position = \"dodge\") +\n scale_fill_brewer(palette = \"Set1\") +\n labs(\n    x = \"Homeownership\", y = \"Count\", fill = \"Application type\",\n    title = \"Dodged bar chart\"\n )\nDodged bar charts are more agnostic in their display about which variable, if any, represents the explanatory and which is the response variable. It is also easy to discern the number of cases in the six group combinations. However, one downside is that it tends to require more horizontal space. Additionally, when two groups are of very different sizes, as we see in the group own relative to either of the other two groups, it is difficult to discern if there is an association between the variables.\nA third option for the position argument is fill. Using this option makes it easy to compare the distribution within one group over all groups in the dataset. But we have no idea about the sizes of the different groups.\nggplot(loans, aes(x = homeownership, fill = application_type)) +\n geom_bar(position = \"fill\") +\n scale_fill_brewer(palette = \"Set1\") +\n labs(\n     x = \"Homeownership\", y = \"Count\", fill = \"Application type\",\n     title = \"Filled bar chart\"\n     )\n\n\n\n\nA mosaic plot is a visualization technique for contingency tables that is similar to a filled bar chart with the benefit that we still see the relative group sizes. We can use geom_mosaic() from the ggmosaic package to create such a plot\n\nlibrary(ggmosaic)\n ggplot(loans) +\n geom_mosaic(aes(x = product(homeownership), fill = application_type)) +\n scale_fill_brewer(palette = \"Set1\") +\n labs(x = \"Homeownership\", y = \"Application type\") +\n guides(fill = \"none\") # no legend\nEach column represents a level of homeownership, and the column widths correspond to the proportion of loans in each category.\nMosaic plots use box areas to represent the number of cases in each category. We can again use this plot to see that the homeownership and application_type variables are associated since some columns are divided in different vertical locations than others.\n\n\n\n\nPie charts can work for visualizing a categorical variable with very few levels.\n\nloans |&gt;\n    count(homeownership) |&gt;\n    ggplot(aes(x=\"\", y=n, fill=homeownership)) +\n    geom_bar(stat=\"identity\", width=1, color=\"white\") +\n    geom_text(aes(label = n),\n        position = position_stack(vjust = 0.5)) +\n    coord_polar(\"y\", start=0) +\n    theme_void()\nHowever, they can be pretty tricky to read when they are used to visualize a categorical variable with many levels. Hence, it would be best if you never used a pie chart. Use a bar chart instead. But if you really want to, use a waffle chart.\n\n\n\nWaffle charts can be used to communicate the proportion of the data that falls into each level of a categorical variable. We use geom_waffle() from the ggwaffle package to create the chart.\nlibrary(ggwaffle)\nwaffle_data &lt;- waffle_iron(\n    loan50, # sample of size 50\n    aes_d(group = homeownership), rows = 5\n)\n\nggplot(waffle_data,\n       aes(x = x, y = y, fill = group)\n       ) +\n  geom_waffle() +\n  coord_equal() +\n  scale_fill_waffle() +\n  theme_waffle() +\n  labs(x = \"\", y = \"\")"
  },
  {
    "objectID": "Lecture-summaries/Experiments-explore-data.html#experiments-explore-data",
    "href": "Lecture-summaries/Experiments-explore-data.html#experiments-explore-data",
    "title": "",
    "section": "",
    "text": "The used Data Set is: loans_full_schema which is contained in the openintro package and includes thousands of loans made through the Lending Club, which is a platform that allows individuals to lend to other individuals.\nWe will examine the relationship between the two variables - homeownership, which can take one of the values of rent, mortgage (owns but has a mortgage), or own, and - application_type, which indicates whether the loan application was made with a partner (joint) or whether it was an individual application.\nThe data requires some data cleaning.\nloans &lt;- loans_full_schema |&gt;\n    mutate(\n        # lower case letters\n        homeownership = tolower(homeownership),\n        # pick new levels\n        homeownership = fct_relevel(\n        homeownership,\n        \"rent\", \"mortgage\", \"own\"\n    ),\n    application_type = fct_relevel(\n        as.character(application_type),\n        \"joint\", \"individual\"\n        )\n    )\n\n\n\nWe get the following absolute frequencies for homeownership and application_type.\nloans |&gt;\n    count(homeownership)\nloans |&gt;\n    count(application_type)\nInstead of absolute frequencies, we can compute the relative frequencies (proportions): \\(r_{j} = \\frac {f_{j}}{n}\\)\nloans |&gt;\n    count(homeownership) |&gt;\n    mutate(prop = n / sum(n))\n\n\n\n\nA bar chart is a common way to display the distribution of a single categorical variable. In ggplot2 we can use geom_bar() to create a bar chart.\n\nggplot(loans, aes(x = homeownership)) +\n    geom_bar(fill = \"gold\") +\n    labs(x = \"Homeownership\", y = \"Count\")\n\n\n\nIn the previous plot, we did not present the data as they are. In a preliminary step, absolute frequencies were calculated for homeownership and then these values were plotted.\n\nEach geom has its own set of variables to be calculated. For geom_bar() these are:\n\nComputed variables: These are calculated by the ‘stat’ part of layers and can be accessed with delayed evaluation. - after_stat(count) number of points in bin. - after_stat(prop) groupwise proportion\n\nThe help page of each geom function contains a list with all computed variables, where the first entry is the default computation.\n\nTo create a bar chart of relative frequencies (not absolute), we first have to apply the statistical transformation prop to the whole data set. after_stat(prop) computes groupwise proportions. The data contains three groups concerning homeownership. If we want to calculate proportions for each group with respect to the size of the whole dataset, we first have to assign a common group value (e.g., group = 1) for all three groups.\nggplot(loans,\n aes(x = homeownership,\n     y = after_stat(prop), group = 1)) +\n geom_bar(fill = \"gold\") +\n labs(x = \"Homeownership\")\n\n\n\n\n\n\n\nloans |&gt;\n    select(homeownership, application_type) |&gt;\n    table() \n\n#               application_type\n# homeownership joint individual\n#   rent        362 3496\n#   mortgage    950 3839\n#   own         183 1170\nWe can also add the marginal frequency distributions.\nloans |&gt;\n    select(homeownership, application_type) |&gt;\n    table() |&gt;\n    addmargins()\n\n#               application_type\n# homeownership joint individual Sum\n# rent          362     3496    3858\n# mortgage      950     3839    4789\n# own           183     1170    1353\n# Sum           1495    8505    10000\n\nContingency tables can also be computed using count().\n\n\nprop.table() converts a contingency table with absolute frequencies into a table with proportions.\n\nloans |&gt;\n    select(homeownership, application_type) |&gt;\n    table() |&gt;\n    prop.table()\n\n#               application_type\n# homeownership joint individual\n# rent          0.0362 0.3496\n# mortgage      0.0950 0.3839\n# own           0.0183 0.1170\nTo add row and column proportions, one can use the margin argument.\nFor row proportion (absolute frequencies divided by the row totals), we have to use margin=1\nloans |&gt;\n    select(homeownership, application_type) |&gt;\n    table() |&gt;\n    prop.table(margin = 1) |&gt;\n    addmargins()\n\n#                   application_type\n# homeownership joint     individual Sum\n# rent          0.0938310 0.9061690 1.0000000\n# mortgage      0.1983713 0.8016287 1.0000000\n# own           0.1352550 0.8647450 1.0000000\n# Sum           0.4274573 2.5725427 3.0000000\n\nRow and column proportions can also be thought of as conditional proportions as they tell us about the proportion of observations in a given level of a categorical variable conditional on the level of another categorical variable.\n\n\n\n\n\nWe can plot the distributions of two categorical variables simultaneously in a bar chart. Such charts are generally helpful to visualize the relationship between two categorical variables.\n\nggplot(loans, aes(x = homeownership, fill = application_type)) +\n    geom_bar() +\n    scale_fill_brewer(palette = \"Set1\") +\n    labs(\n        x = \"Homeownership\", y = \"Count\", fill = \"Application type\",\n        title = \"Stacked bar chart\"\n    )\nLoan applicants most often live in homes with mortgages. But it is not so easy to say how the different types of applications differ over the levels of homeownership. The stacked bar chart is most useful when it’s reasonable to assign one variable as the explanatory variable (here homeownership) and the other variable as the response (here application_type) since we are effectively grouping by one variable first and then breaking it down by the others.\nOne can vary the bars’ position with the position argument of geom_bar()\nggplot(loans, aes(x = homeownership, fill = application_type)) +\n geom_bar(position = \"dodge\") +\n scale_fill_brewer(palette = \"Set1\") +\n labs(\n    x = \"Homeownership\", y = \"Count\", fill = \"Application type\",\n    title = \"Dodged bar chart\"\n )\nDodged bar charts are more agnostic in their display about which variable, if any, represents the explanatory and which is the response variable. It is also easy to discern the number of cases in the six group combinations. However, one downside is that it tends to require more horizontal space. Additionally, when two groups are of very different sizes, as we see in the group own relative to either of the other two groups, it is difficult to discern if there is an association between the variables.\nA third option for the position argument is fill. Using this option makes it easy to compare the distribution within one group over all groups in the dataset. But we have no idea about the sizes of the different groups.\nggplot(loans, aes(x = homeownership, fill = application_type)) +\n geom_bar(position = \"fill\") +\n scale_fill_brewer(palette = \"Set1\") +\n labs(\n     x = \"Homeownership\", y = \"Count\", fill = \"Application type\",\n     title = \"Filled bar chart\"\n     )\n\n\n\n\nA mosaic plot is a visualization technique for contingency tables that is similar to a filled bar chart with the benefit that we still see the relative group sizes. We can use geom_mosaic() from the ggmosaic package to create such a plot\n\nlibrary(ggmosaic)\n ggplot(loans) +\n geom_mosaic(aes(x = product(homeownership), fill = application_type)) +\n scale_fill_brewer(palette = \"Set1\") +\n labs(x = \"Homeownership\", y = \"Application type\") +\n guides(fill = \"none\") # no legend\nEach column represents a level of homeownership, and the column widths correspond to the proportion of loans in each category.\nMosaic plots use box areas to represent the number of cases in each category. We can again use this plot to see that the homeownership and application_type variables are associated since some columns are divided in different vertical locations than others.\n\n\n\n\nPie charts can work for visualizing a categorical variable with very few levels.\n\nloans |&gt;\n    count(homeownership) |&gt;\n    ggplot(aes(x=\"\", y=n, fill=homeownership)) +\n    geom_bar(stat=\"identity\", width=1, color=\"white\") +\n    geom_text(aes(label = n),\n        position = position_stack(vjust = 0.5)) +\n    coord_polar(\"y\", start=0) +\n    theme_void()\nHowever, they can be pretty tricky to read when they are used to visualize a categorical variable with many levels. Hence, it would be best if you never used a pie chart. Use a bar chart instead. But if you really want to, use a waffle chart.\n\n\n\nWaffle charts can be used to communicate the proportion of the data that falls into each level of a categorical variable. We use geom_waffle() from the ggwaffle package to create the chart.\nlibrary(ggwaffle)\nwaffle_data &lt;- waffle_iron(\n    loan50, # sample of size 50\n    aes_d(group = homeownership), rows = 5\n)\n\nggplot(waffle_data,\n       aes(x = x, y = y, fill = group)\n       ) +\n  geom_waffle() +\n  coord_equal() +\n  scale_fill_waffle() +\n  theme_waffle() +\n  labs(x = \"\", y = \"\")"
  },
  {
    "objectID": "Lecture-summaries/Data-and-visualization.html",
    "href": "Lecture-summaries/Data-and-visualization.html",
    "title": "",
    "section": "",
    "text": "In a dataset, each row is an observation and each column is a variable.\n\n\n\nExploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics. Often, EDA is visual – this is what we’ll focus on first. But we might also calculate summary statistics and perform data transformation at (or before) this analysis stage – this is what we’ll focus on next.\n\n\n\nData visualization is the creation and study of the visual representation of data.\nMany tools exist for visualizing data, and R is one of them.\n\nggplot2 is tidyverse’s data visualization package, i.e., ggplot2 is loaded after running. The gg stands for Grammar of Graphics: tool that enables us to concisely describe the components of a graphic.\n\nlibrary(tidyverse)\nLet’s look at the plot of mass vs. height of Star Wars characters:\nggplot(\n    data= starwars,\n    mapping = aes(x = height, y = mass)\n) + \ngeom_point() +\nlabs(\n    title = \"Mass vs. height of Starwars characters\",\n    x=\"Height (cm)\", y=\"Weight(kg)\"\n)\n\n\nggplot() is the main function in ggplot2. It initializes the plot. The different layers of the plots are then added consecutively.\n\n\n\n\n\nThe function aes() creates the mapping from the dataset variables to the plot’s aesthetics.\nRepresent each observation with a point by using geom_point().\nMap species to the colour of each observation point.\nTitle the plot “Bill depth and length”.\nAdd the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”. Label the x- and y-axis as “Bill depth (mm)” and “Bill length (mm)”, respectively\nLabel the legend “Species”\nAdd a caption for the data source.\nuse a discrete colour scale to be perceived by viewers with common colour blindness. (+ scale_colour_viridis_d())\n\nlibrary(palmerpenguins)\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     colour = species)) + \n    geom_point() +\n    labs(\n      title = \"Bill depth and length\",\n      subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n      x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n      colour = \"Species\",\n      caption = \"Source: Palmer Station LTER/palmerpenguins package\")\n\n\n\n\n\nCommonly used aesthetics of a graphic are colour, shape, size or alpha (transparency)\nRemark. The values of shape can only be specified by a discrete variable. Using instead a continuous variable will lead to an error.\n\n\n\n\n Mapping: Determine the size, alpha, etc., of the geometric objects, like points, based on the values of a variable in the dataset.\nUse aes() to define the mapping.  Setting: Determine the size, alpha, etc., of the geometric objects, like points, not based on the values of a variable in the dataset.\nSpecify the aesthetics within geom_*()\n\n\nmapping:\nggplot(\n penguins,\n aes(\n  x = bill_depth_mm,\n  y = bill_length_mm,\n  size = body_mass_g,\n  alpha = flipper_length_mm)) +\n geom_point()\nsetting:\nggplot(penguins,\n  aes( x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(\n  size = 2,\n  alpha = 0.5)\n\n\n\n\nFaceting means creating smaller plots that display different subsets of the data. Useful for exploring conditional relationships and large data.\n\n\n facet_grid():\n\n\nable to create 2d grid\n\n\nrows ~ cols\n\n\nuse . for no split in rows or columns\n\n\nfacet_wrap(): 1d ribbon wrapped according to the number of rows and columns specified or available plotting area\n\n\nFacets based on variables defining aesthetics When facets are built based on a variable used for coloring, the output will contain an unnecessary legend.\nThe information about the different species is already shown on the y-axis and, hence, doesn’t need to be repeated in the legend. One can remove the legend using either guides(), or theme(legend.position = “none”).\nggplot(\n  penguins,\n  aes(x = bill_depth_mm, y = bill_length_mm,\n    color = species)) +\n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d() +\n  guides(color = FALSE)\n\n\n\n\nOther geoms are applied analogously to geom_point(). One can also combine several geoms in one plot.\nDifferent geoms describe different aspects of the data, and the choice of the appropriate geom also depends on the type of the data.\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm,\n  color = species)) +\n  geom_point() +\n  geom_smooth()"
  },
  {
    "objectID": "Lecture-summaries/Data-and-visualization.html#data-and-visualization",
    "href": "Lecture-summaries/Data-and-visualization.html#data-and-visualization",
    "title": "",
    "section": "",
    "text": "In a dataset, each row is an observation and each column is a variable.\n\n\n\nExploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics. Often, EDA is visual – this is what we’ll focus on first. But we might also calculate summary statistics and perform data transformation at (or before) this analysis stage – this is what we’ll focus on next.\n\n\n\nData visualization is the creation and study of the visual representation of data.\nMany tools exist for visualizing data, and R is one of them.\n\nggplot2 is tidyverse’s data visualization package, i.e., ggplot2 is loaded after running. The gg stands for Grammar of Graphics: tool that enables us to concisely describe the components of a graphic.\n\nlibrary(tidyverse)\nLet’s look at the plot of mass vs. height of Star Wars characters:\nggplot(\n    data= starwars,\n    mapping = aes(x = height, y = mass)\n) + \ngeom_point() +\nlabs(\n    title = \"Mass vs. height of Starwars characters\",\n    x=\"Height (cm)\", y=\"Weight(kg)\"\n)\n\n\nggplot() is the main function in ggplot2. It initializes the plot. The different layers of the plots are then added consecutively.\n\n\n\n\n\nThe function aes() creates the mapping from the dataset variables to the plot’s aesthetics.\nRepresent each observation with a point by using geom_point().\nMap species to the colour of each observation point.\nTitle the plot “Bill depth and length”.\nAdd the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”. Label the x- and y-axis as “Bill depth (mm)” and “Bill length (mm)”, respectively\nLabel the legend “Species”\nAdd a caption for the data source.\nuse a discrete colour scale to be perceived by viewers with common colour blindness. (+ scale_colour_viridis_d())\n\nlibrary(palmerpenguins)\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     colour = species)) + \n    geom_point() +\n    labs(\n      title = \"Bill depth and length\",\n      subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n      x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n      colour = \"Species\",\n      caption = \"Source: Palmer Station LTER/palmerpenguins package\")\n\n\n\n\n\nCommonly used aesthetics of a graphic are colour, shape, size or alpha (transparency)\nRemark. The values of shape can only be specified by a discrete variable. Using instead a continuous variable will lead to an error.\n\n\n\n\n Mapping: Determine the size, alpha, etc., of the geometric objects, like points, based on the values of a variable in the dataset.\nUse aes() to define the mapping.  Setting: Determine the size, alpha, etc., of the geometric objects, like points, not based on the values of a variable in the dataset.\nSpecify the aesthetics within geom_*()\n\n\nmapping:\nggplot(\n penguins,\n aes(\n  x = bill_depth_mm,\n  y = bill_length_mm,\n  size = body_mass_g,\n  alpha = flipper_length_mm)) +\n geom_point()\nsetting:\nggplot(penguins,\n  aes( x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(\n  size = 2,\n  alpha = 0.5)\n\n\n\n\nFaceting means creating smaller plots that display different subsets of the data. Useful for exploring conditional relationships and large data.\n\n\n facet_grid():\n\n\nable to create 2d grid\n\n\nrows ~ cols\n\n\nuse . for no split in rows or columns\n\n\nfacet_wrap(): 1d ribbon wrapped according to the number of rows and columns specified or available plotting area\n\n\nFacets based on variables defining aesthetics When facets are built based on a variable used for coloring, the output will contain an unnecessary legend.\nThe information about the different species is already shown on the y-axis and, hence, doesn’t need to be repeated in the legend. One can remove the legend using either guides(), or theme(legend.position = “none”).\nggplot(\n  penguins,\n  aes(x = bill_depth_mm, y = bill_length_mm,\n    color = species)) +\n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d() +\n  guides(color = FALSE)\n\n\n\n\nOther geoms are applied analogously to geom_point(). One can also combine several geoms in one plot.\nDifferent geoms describe different aspects of the data, and the choice of the appropriate geom also depends on the type of the data.\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm,\n  color = species)) +\n  geom_point() +\n  geom_smooth()"
  }
]